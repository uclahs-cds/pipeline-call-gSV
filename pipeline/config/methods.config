methods {
    check_permissions = { path ->
        def filePath = new File(path)

        if (filePath.exists()) {
            if (filePath.canWrite()) {
                return
                }
            throw new Exception("${path} is not writable")
            }

        // Attempts to create directory if the path does not exist
        if (!filePath.mkdirs()) {
            throw new Exception("${path} does not exist and could not create")
            }
        }

    set_log_output_dir = {

        def sample

        // assumes that project and samples name are in the pipeline.config
        def reader = new FileReader(params.input_csv)
        reader.splitEachLine(',') { parts -> [sample = parts[1]] }

        tz = TimeZone.getTimeZone('UTC')
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        params.dataset_registry_prefix = '/hot/data'

        if (params.blcds_registered_dataset == true) {
            if ("${params.dataset_id.length()}" != 11) {
                 throw new Exception("Dataset id must be eleven characters long")
                }
            def disease = "${params.dataset_id.substring(0,4)}"
            // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
            params.log_output_dir = "${params.dataset_registry_prefix}/$disease/${params.dataset_id}/${project}/${sample}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/$date"
            params.disease = "${disease}"
            }
        else {
            params.log_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${sample}/log-${manifest.name}-${manifest.version}-${date}"
            params.disease = null
            }

        params.sample = "${sample}"
        params.date = "${date}"
        }

    set_output_dir = {
        params.output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample}"
        }

    // Function to ensure that resource requirements don't go beyond
    // a maximum limit
    check_max = { obj, type ->
        if (type == 'memory') {
            try {
                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                    return params.max_memory as nextflow.util.MemoryUnit
                else
                    return obj
                } 
            catch (all) {
                println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
                return obj
                }
            } 
        else if (type == 'time') {
            try {
                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                    return params.max_time as nextflow.util.Duration
                else
                    return obj
                } 
            catch (all) {
                println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
                return obj
                }
            } 
        else if (type == 'cpus') {
            try {
                return Math.min(obj, params.max_cpus as int)
                } 
            catch (all) {
                println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
                return obj
                }
            }
        }

    set_resources_allocation = {
        // Function to ensure that resource requirements don't go beyond
        // a maximum limit        
        node_cpus = params.max_cpus
        node_memory_GB = params.max_memory.toGiga()
        // Load base.config by default for all pipelines
        includeConfig "${projectDir}/config/base.config"
        if (params.ucla_cds) {
            if (node_cpus == 64) {
                // Check memory for M64 node
                if (node_cpus == 64 && node_memory_GB >= 950 && node_memory_GB <= 1010) {
                    includeConfig "${projectDir}/config/M64.config"
                    }
                else {
                    throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
                    }
                }
            else {
                // Check memory for F series node
                if (node_memory_GB >= (node_cpus * 2 * 0.9) && node_memory_GB <= (node_cpus * 2)) {
                    includeConfig "${projectDir}/config/F${node_cpus}.config"
                    }
                else {
                    throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
                    }
                }
            }
        }


    /**
     * Check the permissions and existence of workDir.
     * If it doesn't exist, recursively find first existing directory and check write permission.
     * If it exists, check write permission.
     */
    check_workdir_permissions = { dir ->
        dir_file = new File(dir)
        if (dir_file.exists()) {
            if (dir_file.canWrite()) {
                return true
                }
            else {
                throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} is not writeable. Please verify and try again.")
                }
            }
        else {
            while (!dir_file.exists()) {
                dir_file = dir_file.getParentFile()
                }

            if (dir_file.canWrite()) {
                return true
                }
            else {
                throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} cannot be created. The closest existing parent directory ${dir_file.toString()} is not writable. Please verify permissions or change the input parameter.")
                }
            }
        }

    // Location of Nextflow temp directories
    set_env = {
        if (params.ucla_cds) {
            /**
             * By default, if the /scratch directory exists, set it as the Nextflow working directory
             * If config file specified work_dir, set it as the Nextflow working directory
             *
             * WARNING: changing this directory can lead to high server latency and
             * potential disk space limitations. Change with caution! The 'workDir'
             * in Nextflow determines the location of intermediate and temporary files.
             */
            params.work_dir = (params.containsKey("work_dir") && params.work_dir) ? params.work_dir : "/scratch"
            if (methods.check_workdir_permissions(params.work_dir)) {
                workDir = params.work_dir
                }
            }
        else {
            // If work_dir was specified as a param and exists or can be created, set workDir. Otherwise, let Nextflow's default behavior dictate workDir
            if (params.containsKey("work_dir") && params.work_dir && methods.check_workdir_permissions(params.work_dir)) {
                workDir = params.work_dir
                }
            }
        }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"
        
        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_process = {
        process.cache = params.cache_intermediate_pipeline_steps
    }

    set_docker_sudo = {
        if (params.containsKey("blcds_cluster_slurm") && (!params.blcds_cluster_slurm)) {
            docker.sudo = true
        }
    }

    // Set up env, timeline, trace, and report above.
    setup = {
        methods.set_log_output_dir()
        methods.set_output_dir()
        methods.check_permissions(params.log_output_dir)
        
        methods.set_env()
        methods.set_resources_allocation()
        methods.set_process()
        methods.set_docker_sudo()
        methods.set_pipeline_logs()
        }
    }

