methods {
    set_log_output_dir = {

        def patient
        def sample

        // assumes that patient and samples name are in the pipeline.config
        def reader = new FileReader(params.input_csv)
        reader.splitEachLine(',') { parts -> [patient = parts[0], sample = parts[1]] }

        def date = new Date().format('yyyyMMdd-HHMMss')
        if (params.sge_scheduler) {
              params.avere_prefix = '/data/data'
        } else {
              params.avere_prefix = '/hot/data'
        }

        if (params.blcds_registered_dataset == true) {
            if ("${params.dataset_id.length()}" != 11) {
                 throw new Exception("Dataset id must be eleven characters long")
            }
            def disease = "${params.dataset_id.substring(0,4)}"
            // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
            params.output_log_directory = "${params.avere_prefix}/$disease/${params.dataset_id}/${patient}/${sample}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/$date"
            params.disease = "${disease}"
        } else {
            params.output_log_directory = "${params.output_dir}/$date/logs/"
            params.disease = null
        }
        params.patient = "${patient}"
        params.sample = "${sample}"
        params.date = "${date}"
    }
}

methods.set_log_output_dir()

timeline {
   enabled = true
    file = "${params.output_log_directory}/timeline.html"
}

trace {
    enabled = true
    file = "${params.output_log_directory}/trace.txt"
}

report {
    enabled = true
    file = "${params.output_log_directory}/report.html"
}

docker {
    enabled = true
}

if (params.sge_scheduler) {
    docker.sudo = true
}

// Process specific scope
process {
    executor = "local"
}

workDir = ${params.temp_dir}
NXF_WORK = ${params.temp_dir}
NXF_TEMP = ${params.temp_dir}
NXF_HOME = ${params.temp_dir}
