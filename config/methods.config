import nextflow.util.SysHelper
includeConfig "../external/pipeline-Nextflow-config/config/bam/bam_parser.config"
includeConfig "../external/pipeline-Nextflow-config/config/methods/common_methods.config"
includeConfig "../external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "../external/pipeline-Nextflow-config/config/retry/retry.config"

methods {
    set_ids_from_bams = {
        params.sample_to_process = [] as Set
        params.input.BAM.each { k, v ->
            v.each { bam_path ->
                def bam_header = bam_parser.parse_bam_header(bam_path)
                def sm_tags = bam_header['read_group'].collect{ it['SM'] }.unique()

                if (sm_tags.size() != 1) {
                    throw new Exception("${bam_path} contains multiple samples! Please run pipeline with a single sample BAM.")
                    }
                params.sample_to_process.add(['id': sm_tags[0], 'path': bam_path, 'sample_type': k])
                }
            }
        }

    set_output_dir = {
        def sample = params.sample_to_process
            .collect{ it.id }

        if (sample.size() != 1) {
            throw new Exception("${params.sample_to_process}\n\n Multiple BAMs found in the input! Please run pipeline one sample at a time.")
        }

        params.sample = sample[0]

        params.output_dir_base = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample}"
        }

    set_log_output_dir = {

        tz = TimeZone.getTimeZone('UTC')
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        params.dataset_registry_prefix = '/hot/data'

        if (params.blcds_registered_dataset == true) {
            if ("${params.dataset_id.length()}" != 11) {
                 throw new Exception("Dataset id must be eleven characters long")
                }
            def disease = "${params.dataset_id.substring(0,4)}"
            // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
            params.log_output_dir = "${params.dataset_registry_prefix}/${disease}/${params.dataset_id}/${project}/${params.sample}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/${date}"
            params.disease = "${disease}"
            }
        else {
            params.log_output_dir = "${params.output_dir_base}/log-${manifest.name}-${manifest.version}-${date}"
            params.disease = null
            }

        params.date = "${date}"
        }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_process = {
        process.cache = params.cache_intermediate_pipeline_steps
    }

    modify_base_allocations = {
        if (!(params.containsKey('base_resource_update') && params.base_resource_update)) {
            return
        }

        params.base_resource_update.each { resource, updates ->
            updates.each { processes, multiplier ->
                def processes_to_update = (custom_schema_types.is_string(processes)) ? [processes] : processes
                methods.update_base_resource_allocation(resource, multiplier, processes_to_update)
            }
        }
    }

    // Set up env, timeline, trace, and report above.
    setup = {
        methods.set_env()
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate()
        methods.set_ids_from_bams()
        methods.set_output_dir()
        methods.set_log_output_dir()
        methods.set_pipeline_logs()
        methods.set_process()
        methods.setup_process_afterscript()
        methods.setup_docker_cpus()
        resource_handler.handle_resources("${projectDir}/config/resources.json")
        }
    }
