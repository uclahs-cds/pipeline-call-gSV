import nextflow.util.SysHelper
includeConfig "../external/pipeline-Nextflow-config/config/bam/bam_parser.config"
includeConfig "../external/pipeline-Nextflow-config/config/methods/common_methods.config"
includeConfig "../external/pipeline-Nextflow-config/config/schema/schema.config"

methods {
    check_permissions = { path ->
        def filePath = new File(path)

        if (filePath.exists()) {
            if (filePath.canWrite()) {
                return
                }
            throw new Exception("${path} is not writable")
            }

        // Attempts to create directory if the path does not exist
        if (!filePath.mkdirs()) {
            throw new Exception("${path} does not exist and could not create")
            }
        }

    set_ids_from_bams = {
        params.sample_to_process = [] as Set
        params.input.BAM.each { k, v ->
            v.each { bam_path ->
                def bam_header = bam_parser.parse_bam_header(bam_path)
                def sm_tags = bam_header['read_group'].collect{ it['SM'] }.unique()

                if (sm_tags.size() != 1) {
                    throw new Exception("${bam_path} contains multiple samples! Please run pipeline with a single sample BAM.")
                    }
                params.sample_to_process.add(['id': sm_tags[0], 'path': bam_path, 'sample_type': k])
                }
            }
        }

    set_output_dir = {
        def sample = params.sample_to_process
            .collect{ it.id }

        if (sample.size() != 1) {
            throw new Exception("${params.sample_to_process}\n\n Multiple BAMs found in the input! Please run pipeline one sample at a time.")
        }

        params.sample = sample[0]

        params.output_dir_base = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample}"
        }

    set_log_output_dir = {

        tz = TimeZone.getTimeZone('UTC')
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        params.dataset_registry_prefix = '/hot/data'

        if (params.blcds_registered_dataset == true) {
            if ("${params.dataset_id.length()}" != 11) {
                 throw new Exception("Dataset id must be eleven characters long")
                }
            def disease = "${params.dataset_id.substring(0,4)}"
            // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
            params.log_output_dir = "${params.dataset_registry_prefix}/${disease}/${params.dataset_id}/${project}/${params.sample}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/${date}"
            params.disease = "${disease}"
            }
        else {
            params.log_output_dir = "${params.output_dir_base}/log-${manifest.name}-${manifest.version}-${date}"
            params.disease = null
            }

        params.date = "${date}"
        }

    // Function to ensure that resource requirements don't go beyond
    // a maximum limit
    check_max = { obj, type ->
        if (type == 'memory') {
            try {
                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                    return params.max_memory as nextflow.util.MemoryUnit
                else
                    return obj
                }
            catch (all) {
                println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
                return obj
                }
            }
        else if (type == 'time') {
            try {
                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                    return params.max_time as nextflow.util.Duration
                else
                    return obj
                }
            catch (all) {
                println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
                return obj
                }
            }
        else if (type == 'cpus') {
            try {
                return Math.min(obj, params.max_cpus as int)
                }
            catch (all) {
                println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
                return obj
                }
            }
        }

    set_resources_allocation = {
        // Function to ensure that resource requirements don't go beyond
        // a maximum limit
        node_cpus = params.max_cpus
        node_memory_GB = params.max_memory.toGiga()
        // Load base.config by default for all pipelines
        includeConfig "${projectDir}/config/base.config"
        if (params.ucla_cds) {
            if (node_cpus == 64) {
                // Check memory for M64 node
                if (node_cpus == 64 && node_memory_GB >= 950 && node_memory_GB <= 1010) {
                    includeConfig "${projectDir}/config/M64.config"
                    }
                else {
                    throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
                    }
                }
            else {
                // Check memory for F series node
                if (node_memory_GB >= (node_cpus * 2 * 0.9 - 1) && node_memory_GB <= (node_cpus * 2)) {
                    includeConfig "${projectDir}/config/F${node_cpus}.config"
                    }
                else {
                    throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
                    }
                }
            }
        }


    /**
     * Check the permissions and existence of workDir.
     * If it doesn't exist, recursively find first existing directory and check write permission.
     * If it exists, check write permission.
     */
    check_workdir_permissions = { dir ->
        dir_file = new File(dir)
        if (dir_file.exists()) {
            if (dir_file.canWrite()) {
                return true
                }
            else {
                throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} is not writeable. Please verify and try again.")
                }
            }
        else {
            while (!dir_file.exists()) {
                dir_file = dir_file.getParentFile()
                }

            if (dir_file.canWrite()) {
                return true
                }
            else {
                throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} cannot be created. The closest existing parent directory ${dir_file.toString()} is not writable. Please verify permissions or change the input parameter.")
                }
            }
        }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_process = {
        process.cache = params.cache_intermediate_pipeline_steps
    }

    set_docker_sudo = {
        if (params.containsKey("blcds_cluster_slurm") && (!params.blcds_cluster_slurm)) {
            docker.sudo = true
        }
    }

    // Set up env, timeline, trace, and report above.
    setup = {
        methods.set_env()
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate()
        methods.set_ids_from_bams()
        methods.set_resources_allocation()
        methods.set_output_dir()
        methods.set_log_output_dir()
        methods.check_permissions(params.log_output_dir)
        methods.set_pipeline_logs()
        methods.set_process()
        methods.set_docker_sudo()
        }
    }
